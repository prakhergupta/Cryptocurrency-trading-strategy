{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ScrapyNews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d5e2184fc865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mScrapyNews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNewsItem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ScrapyNews'"
     ]
    }
   ],
   "source": [
    "from scrapy import Spider\n",
    "from scrapy import Selector\n",
    "from scrapy.http import Request\n",
    "from datetime import date,timedelta\n",
    "from ScrapyNews.items import NewsItem\n",
    "import re \n",
    "\n",
    "class BloombergSpider(Spider):\n",
    "    name = 'bloombergspider'\n",
    "    allowed_domains = ['bloomberg.com']\n",
    "\n",
    "    start_urls = [\n",
    "        \"http://www.bloomberg.com\",\n",
    "        \"http://www.bloomberg.com/markets\",\n",
    "        \"http://www.bloomberg.com/insights\",\n",
    "        \"http://www.bloomberg.com/live\",\n",
    "    ]\n",
    "\n",
    "    parsed_urls = []\n",
    "\n",
    "    start_time = \"2015-04-10\" #date(2015,04,10)\n",
    "    end_time = \"2015-05-31\" #date(2015,05,31)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BloombergSpider,self).__init__()\n",
    "        self.bloomberg_news_url = \"http://www.bloomberg.com/archive/news/\"\n",
    "        self.bloomberg_url = 'http://www.bloomberg.com/'\n",
    "        self.date_pattern = re.compile('([0-9]{4}-[0-9]{2}-[0-9]{2})')\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        ## article/h1/a  or article/a\n",
    "        #//article[@data-tracker-name=\"story\"]\n",
    "\n",
    "        for href in response.xpath('//a[@data-resource-type=\"article\"]/@href'):\n",
    "            url = href.extract()\n",
    "            if not url.startswith(\"http://\"):\n",
    "                url =  self.bloomberg_url + url\n",
    "\n",
    "            # time filtering\n",
    "#            post_date = self.date_pattern.search(url)\n",
    "#            if post_date is not None:\n",
    "#                curr_date = post_date.group(1)\n",
    "#                if curr_date < self.start_time:\n",
    "#                    continue\n",
    "#                if curr_date > self.end_time:\n",
    "#                    continue\n",
    "\n",
    "#            print url\n",
    "\n",
    "            yield Request(url, callback=self.parse_news)\n",
    "\n",
    "        for href in response.xpath('//a[@class=\"navigation-submenu__category-link\"]/@href'):\n",
    "            url = href.extract()\n",
    "            if not url.startswith(\"http://\"):\n",
    "                url =  self.bloomberg_url + url\n",
    "\n",
    "            if url in self.parsed_urls:\n",
    "                continue\n",
    "            self.parsed_urls.append(url)\n",
    "\n",
    "            yield Request(url, callback=self.parse)\n",
    "\n",
    "\n",
    "\n",
    "    def parse_news(self,response):\n",
    "        debug = False\n",
    "\n",
    "        if response.url in self.parsed_urls:\n",
    "            return\n",
    "        self.parsed_urls.append(response.url)\n",
    "        \n",
    "        if debug:\n",
    "            print (\"********************************************************************\")\n",
    "            print (response.url)\n",
    "\n",
    "        newsItem = NewsItem()\n",
    "        hxs = Selector(response)\n",
    "        try:\n",
    "            newsItem['news_title'] = hxs.xpath('//h1[@class=\"lede-headline\"]/span/text()').extract()[0].encode(\"utf-8\")\n",
    "            article_info = hxs.xpath('//div[@class=\"article-details\"]')\n",
    "\n",
    "            # arr-format\n",
    "            newsItem['news_authors'] = article_info.xpath('//a[@class=\"author-link\"]//text()').extract()\n",
    "\n",
    "            newsItem['news_posttime'] = article_info.xpath('//div[@class=\"published-info\"]/time/text()').extract()[0].encode(\"utf-8\")\n",
    "\n",
    "            # arr-format\n",
    "            newsItem['news_content'] = hxs.xpath('//div[@class=\"article-body__content\"]/p//text()').extract()\n",
    "\n",
    "\n",
    "            newsItem['news_url'] = response.url\n",
    "            newsItem['news_filename'] = response.url.rsplit('/',1)[1].strip(\".html\")\n",
    "\n",
    "            post_date = self.date_pattern.search(response.url)\n",
    "            \n",
    "            if post_date is not None:\n",
    "                newsItem['news_dir'] = post_date.group(1)\n",
    "\n",
    "            newsItem['news_domain'] = \"bloomberg\"\n",
    "\n",
    "            if debug:\n",
    "                print (\"title\", newsItem['news_title'])\n",
    "                print (\"authors\", newsItem['news_authors'])\n",
    "                print (\"posttime\", newsItem['news_posttime'])\n",
    "                print (\"content\", \"\".join(newsItem['news_content'])[:20])\n",
    "                print (\"dir\", newsItem[\"news_dir\"])\n",
    "                print (\"filename\", newsItem[\"news_filename\"])\n",
    "\n",
    "            yield newsItem\n",
    "        except Exception as e:\n",
    "            print ('[ERROR]',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-25-7e15caf09b19>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-7e15caf09b19>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    df= read_csv(\"C:\\Users\\gupta\\Downloads\\Big Data\\reddit_worldnews_start_to_2016-11-22.csv\")\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "df= read_csv(\"C:\\Users\\gupta\\Downloads\\Big Data\\reddit_worldnews_start_to_2016-11-22.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
